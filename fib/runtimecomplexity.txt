Runtime Complexity, Big 'O' Notation
Constant time = 1
  - no matter how many elements we're working with, the algorithm/operation/whatever will always take the same amount of time
  - rarely will such an algorithm exist, its the optimal situation.
  - the holy grail of solutions
Logarithmic time = log(n)
  - you have this if doubling the number of elements you are iterating over doesnt double the amount of work.
  - always assume that searching operations are log(n)
Linear Time = n
  - iterating through all elements in a collection of data. 
  - if you see a for loop spanning from '0' to 'array.length', you probably have 'n', or linear runtime
Quasilinear Time = n * log(n)
  - you have this if doubling the number of elements you are iterating over doesnt double the amount of work.
  - always assume that any sorting operation is n*log(n)
Quadratic Time = n ^ 2
  - every element in a collection has to be compared to every other element. 'the handshake problem'
Exponential Time = 2 ^ n
  - if you add a 'single' element to a collection the processing power required doubles.



Identifying Runtime Complexity
iterating with a simple for loop through a single collection?
  Probably O(n)
Iterating through half a colleciton? half a string...
  still O(n). there are no constants in runtime...
Iterating through two 'different' collections with separate for loops? iterate two strings
  O(n + m) ... n represents the 1st string, m represents the 2nd string.
Two nested for loops iterating over the same collection? nested for loops, big red flag
  O(n ^ 2)
  the steps algorithm, and the pyramid algorithm, specifically the iterative solutions
Two nested for loops iterating over different collections?
  O(n * m) 
Sorting?
  O(n * log(n))
Searching a sorted array?
  O(log(n))

Space complexity is similar to runtime complexity... the math is slightly different.
  how much memory is wrequire by doubling the problem set?


Notes on studying:

You're trying to memorize which big-O expression goes with a given algorithmic structure, 
but you should really just count up the number of operations that the algorithm requires 
and compare that to the size of the input. An algorithm that loops over its entire input 
has O(n) performance because it runs the loop n times, not because it has a single loop. 
Here's a single loop with O(log n) performance:

for (i = 0; i < log2(input.count); i++) {
    doSomething(...);
}
So, any algorithm where the number of required operations is on the order of the logarithm of the size of the input is O(log n). 
The important thing that big-O analysis tells you is how the execution time of an algorithm changes relative to the size of the input: 
if you double the size of the input, does the algorithm take 1 more step (O(log n)), twice as many steps (O(n)), four times as many 
steps (O(n^2)), etc.

